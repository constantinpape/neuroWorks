# neuroWorks

Neural network babysteps with tensorflow

## Some ideas:

* Prunining for NNs start with fully connected NN and drop connections that become too weak.
Basically that is L0 (infeasible) / L1 loss <- NN with L1 papers? But could also drop connections once their absolute value becomes too small.
Probably with droping rate increasing during the learning process?
* End-to-end learning for connectomics

## Side projects:

* DeepQuoran
